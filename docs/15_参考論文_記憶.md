# 参考論文メモ（記憶・長文コンテキスト）

作成日: 2026-01-25

このドキュメントは、CocoroGhost（AI人格システムのLLM/記憶バックエンド）の現行設計を踏まえ、提示された論文から「本プロジェクトで参考にできる点」を抽出して整理するメモです。

## 前提: CocoroGhost（本リポジトリ）の現状整理（要点）

### 目的と基本構造

- 目的は「忘れないログ」と「育つ状態」を分けて、人間っぽい想起（連想/目的検索）を実現すること。
- 永続の中心は以下（詳細は `docs/01_全体像.md` / `docs/02_記憶モデル.md`）:
  - `events`: 会話/通知/観測などの出来事ログ（追記、基本削除しない）
  - `state`: 将来の会話に効く状態（fact/relation/task/summary/long_mood_state 等）
  - `revisions`: 状態などの更新履歴（根拠イベント必須の設計）
  - `event_links` / `event_threads`: 文脈グラフ（同期は軽量仮置き、非同期で本更新）
  - `event_affects` / `long_mood_state`: 感情（瞬間ログ + 長期背景）

### 同期（/api/chat）: 「検索→選別→SSE返答」

- 返答開始までの体感速度を優先し、ストリーム開始前に「必要な記憶」を確定する（`docs/10_実行フロー.md`）。
- ざっくり流れ:
  - ルールで `RetrievalPlan` を作成（LLMでplan生成しない）
  - DBで候補を広く収集（ベクトル/文字3-gram/文脈グラフ/最近状態など）
  - LLMが候補を最終選別して `SearchResultPack`（内部JSON）を作る
  - LLMが `SearchResultPack` / `TimeContext` / `LongMoodState` を内部注入して返答をSSE生成

### 非同期（worker）: 「WritePlan→適用→索引/整理」

- 返答後に、LLMが `WritePlan`（内部JSON）を生成し、状態/感情/文脈グラフを更新する（`docs/05_記憶更新（育てる）.md`）。
- `WritePlan` の文章はAI人格の口調（persona）に寄せる設計（`cocoro_ghost/prompt_builders.py` / `cocoro_ghost/worker.py`）。

### 人格（persona）

- 人格は `persona_presets.persona_text` と `second_person_label`（二人称呼称）で管理され、返答生成とWritePlan生成のsystem promptに注入される。
- 会話本文向けの追加指示（addon）は別に持ち、WritePlan（内部JSON生成）には注入しない設計。

---

## 論文1: From RAG to Memory: Non-Parametric Continual Learning for Large Language Models（arXiv:2502.14802v2）

参照: `https://arxiv.org/html/2502.14802v2`

### 概要（何を解くか）

- 「LLMに新しい知識を継続的に取り込む」手段としてRAGが主流だが、単純なベクトル検索中心だと、人間の長期記憶っぽい性質（関連づけ/多段推論/文脈）を再現しにくい。
- 既存の「KGなど構造を足したRAG」は、連想/意味理解は上がっても、基本的な事実想起（factual）で標準RAGより落ちるケースがある。
- それを避けつつ、factual / sense-making / associativity の3観点で総合的に強い枠組みとして **HippoRAG 2** を提案。

### 手法の要点（覚えておくべき部品）

- KG（スキーマレス）+ Personalized PageRank（PPR）で多段の関連づけ（associativity）を狙う。
- **Dense-Sparse Integration**:
  - 概念（phrase node）だけだと文脈が落ちるので、**passage node** を導入し、passage→phrase の “contains” 辺で「概念と出典文脈」を同じグラフ上に統合。
- **Deeper Contextualization**:
  - クエリ→KGへの接続を「NER→node」だけに寄せず、クエリ全体を使って node/triple に当てる（特に query-to-triple をデフォルト採用）。
- **Recognition Memory**:
  - embeddingで拾った top-k triple を、LLMでフィルタして seed をきれいにする（誤ったseedを除外する）。
- **Online Retrieval**:
  - フィルタ後triple由来の phrase node と、passage node を seed として PPR を走らせ、PageRankでpassageをランキングして最終コンテキストにする。

### CocoroGhostで参考にできそうな点（検討）

- 本プロジェクトはすでに「複数経路で候補を広く集め、LLMで選別する」設計なので、HippoRAG 2の **recognition memory（LLMフィルタ）** は思想的に相性がよい。
- 一方で、現状の文脈グラフ（`event_links`/`event_threads`）は「会話の流れ」を追うのが主で、**多段の意味連想（associativity）を狙うPPR型のグラフ検索**は未実装（または薄い）。
- 「概念ノード＋文脈ノード（passage/event/state）」の二層をグラフとして扱う発想は、`state(kind=relation)` と `event_links` を統合的に強化する方向性として参考になる。

### 適用案（本プロジェクト向けの落とし込み案）

1) **PPR型の“連想”候補収集経路を追加**  
   - 既存の候補収集に「graph_ppr」系を足し、候補に `hit_sources=["graph_ppr"]` を付けてLLM選別へ渡す。  
   - グラフは「エンティティ（人/物/話題）↔︎ state / event」を中心に構成し、PPRで“距離の近い”出来事や状態を拾う。

2) **「passage node」相当を events/state で作る**  
   - phrase（概念）だけでなく「その概念が現れた出来事/状態」をnodeとして持ち、概念↔︎出来事/状態を “contains” 的にリンクする。

3) **Recognition Memory を“候補圧縮”に使う**  
   - 現状は「候補収集→LLMで選別」をやっているが、選別入力が肥大化すると体感に直撃する。  
   - そこで、選別の手前に軽量なLLMフィルタ（例: “この候補は明らかに関係ない” を落とす）を入れる余地がある。

---

## 論文2: A-Mem: Agentic Memory for LLM Agents（arXiv:2502.12110v11）

参照: `https://arxiv.org/html/2502.12110v11`

### 概要（何を解くか）

- LLMエージェントが長期タスクをやるには、単なる「保存→検索」以上の“整理された記憶”が必要。
- 既存メモリは、固定スキーマ/固定操作（ワークフローに埋め込み）に寄りがちで、タスクや状況が変わると適応しにくい。
- **Zettelkasten**（カード型のノートと柔軟なリンク）を参考に、LLMが自律的に「ノート作成・リンク作成・過去ノートの進化」を行う **agentic memory** を提案。

### 手法の要点

- **Note Construction**:  
  - 1つの記憶を、原文（interaction content）だけでなく、LLM生成のコンテキスト説明/keywords/tags 等を含む構造化ノートとして保存する。
  - さらに embedding を持ち、類似検索の足場にする。
- **Link Generation**:  
  - embeddingで top-k 近傍を引き、LLMで「リンクを張るべきか」を判断してリンク集合（“box”的なクラスター）を作る。
- **Memory Evolution**:  
  - 新しいノートが入ったことをトリガーに、近傍の既存ノートのコンテキスト/keywords/tags をLLMが更新し、知識構造を“育てる”。
- **Retrieve Relative Memory**:
  - クエリも embedding 化して top-k を取り、関連ノートをプロンプトに組み込む（リンクで周辺ノートも辿れる設計）。

### CocoroGhostで参考にできそうな点（検討）

- CocoroGhostは「events（ログ）→state（育つ層）」に分け、LLMが `WritePlan` で更新するので、A-Memの **memory evolution** と非常に近い。
- 一方で、A-Memが強く押している「ノートの属性（keywords/tags/context description）」「ノート間リンクの自動生成」は、現状設計の中心ではない（少なくとも“まずそれ”として明文化されていない）。
- 本プロジェクトの `state.kind="relation"` を、A-Memのノートリンク/クラスタリング発想で強化すると、検索ノイズを増やさずに“関連づけ”を濃くできる可能性がある。

### 適用案（本プロジェクト向けの落とし込み案）

1) **stateを“ノート”として扱う（属性を標準化）**  
   - `state.payload_json` に、最低限 `keywords` / `tags` / `context`（短い説明）を持たせる方針を置く。  
   - `WritePlan` に「keywords/tags を必ず出す」モードを入れるのではなく、必要な kind のみに限定してノイズ増加を抑える（例: fact/relation/summaryだけ）。

2) **ノートリンクの生成を非同期で回す**  
   - 既存のベクトル近傍検索（sqlite-vec）で top-k 近い state を引き、LLMで「リンクする/しない」を判定して保存する。  
   - 保存先は `event_links` とは別概念になるので、（運用前なら） state↔︎state のリンク表を設計するのが自然。

3) **“進化”を revisions 前提で実装する**  
   - CocoroGhostは更新履歴（`revisions`）が核なので、A-Mem的な「既存ノートの書き換え」をやるなら、必ず根拠（どのeventが引き金か）と理由を残す設計に合わせる。

---

## 論文3: A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts（arXiv:2402.09727v3）

参照: `https://arxiv.org/pdf/2402.09727v3`

### 概要（何を解くか）

- LLMはコンテキスト長に限界があり、長文を安定に“読む”のが難しい。
- 人間の読み方を模して、
  - どこで区切るか（episode pagination）
  - 各区切りを短い gist に圧縮する（memory gisting）
  - 必要に応じて原文の該当ページを“読み直す”（interactive look-up）
という、**プロンプトだけで作る読解エージェント**を提案。
- 実験では、gistだけ/検索だけ/原文だけ等のベースラインと比較し、複数の長文QAタスクで有利な結果を示し、実効コンテキストを 3.5〜20 倍に伸ばしたと主張。

### CocoroGhostで参考にできそうな点（検討）

- CocoroGhostはすでに「events→要約（assistant_summary）」「state（summary/fact等）」で圧縮表現を持つが、ReadAgentの肝は **“圧縮した流れ（gist memory）を保ちつつ、必要箇所だけ原文に戻る”** という運用。
- 現状の `/api/chat` はSSE体感優先で多段の追加問い合わせを避けているため、ReadAgentの “interactive look-up” を同期経路にそのまま入れるのは難しい。
- ただし、次のどちらかなら相性が良い:
  - 非同期側で gist を育てる（再利用可能な圧縮表現を作る）
  - 同期側は「まず gist を優先注入」し、必要なら“追加検索”を後段（2ターン目）で自然に行う

### 適用案（本プロジェクト向けの落とし込み案）

1) **文脈スレッド単位の gist（中期要約）を state として育てる**  
   - `event_threads` の thread_key ごとに「最近N件のgist」を非同期で更新し、検索候補として使う。

2) **“差し戻し（読み直し）”の導線を設計する**  
   - 返答が曖昧になりやすい質問（多段・長文参照）だけ、UI/クライアント側が “もう少し調べて” を投げられるようにし、2ターンで look-up を成立させる。

---

## 横断まとめ（結論）

### すぐ参考にできる（設計思想の整合が強い）

- **A-Mem**:
  - stateを“ノート”として捉え、属性（keywords/tags/context）とノート間リンク、そして“進化”を `revisions` 前提で行うのは、CocoroGhostの設計と噛み合う。

### 中期的に効きそう（多段の連想/関連づけの強化）

- **HippoRAG 2**:
  - PPRベースのグラフ検索（associativity）は、現状の候補収集経路に追加する価値がある。
  - ただし、グラフ構築（概念抽出・リンク・重み付け）の設計コストが高いので、まずは「軽いノード/辺設計」から始めるのが現実的。

### 参考にはなるが、そのまま同期経路へは入れにくい

- **ReadAgent**:
  - “gist→必要箇所だけ原文” は強いが、同期SSEの制約と衝突しやすい。非同期で gist を育てる/2ターンで look-up する、という形での採用が現実的。
